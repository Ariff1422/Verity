{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d8b9d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Ensemble Model Evaluation ---\n",
      "Loading all models and data...\n",
      "Loaded 20000 reviews for testing.\n",
      "Generating predictions from base models...\n",
      "Making final predictions with the RandomForest meta-model...\n",
      "\n",
      "--- Evaluation Report ---\n",
      "Overall Ensemble Accuracy on New Data: 0.0000\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28mprint\u001b[39m(report)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[43mtest_ensemble_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mtest_ensemble_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     99\u001b[39m accuracy = accuracy_score(y_true, y_pred)\n\u001b[32m    100\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall Ensemble Accuracy on New Data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m report = \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFull Classification Report:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ariff1422\\Documents\\Verity\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ariff1422\\Documents\\Verity\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2951\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2948\u001b[39m y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n\u001b[32m   2950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2951\u001b[39m     labels = \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2952\u001b[39m     labels_given = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ariff1422\\Documents\\Verity\\venv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:117\u001b[39m, in \u001b[36munique_labels\u001b[39m\u001b[34m(*ys)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMix of label input types (string and number)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n",
      "\u001b[31mValueError\u001b[39m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# --- File Paths and Constants ---\n",
    "# Meta-Model and Vectorizer from the ensemble tuning\n",
    "META_MODEL_PATH = '../models/ensemble_meta_model.pkl'\n",
    "VECTORIZER_PATH = '../models/vectorizer_advanced_tuned.pkl'\n",
    "\n",
    "# Base models (needed for their predictions)\n",
    "GB_MODEL_PATH = '../models/gb_model_advanced_tuned.pkl'\n",
    "# The path to your local TinyBERT model folder\n",
    "TINYBERT_MODEL_PATH = '../models/tinybert_finetuned'\n",
    "\n",
    "# Dataset to be tested\n",
    "TEST_DATA_PATH = '../data/test_labeled_dataset_sampled.csv'\n",
    "\n",
    "def test_ensemble_model():\n",
    "    \"\"\"\n",
    "    Loads the full ensemble pipeline, makes predictions on the pseudo-labeled\n",
    "    test data, and prints the performance report.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Ensemble Model Evaluation ---\")\n",
    "\n",
    "    # --- 1. Load the Necessary Components ---\n",
    "    # First, check if all the necessary files exist\n",
    "    if not os.path.exists(META_MODEL_PATH):\n",
    "        print(f\"Error: The meta-model file '{META_MODEL_PATH}' was not found.\")\n",
    "        return\n",
    "    if not os.path.exists(VECTORIZER_PATH):\n",
    "        print(f\"Error: The vectorizer file '{VECTORIZER_PATH}' was not found.\")\n",
    "        return\n",
    "    if not os.path.exists(GB_MODEL_PATH):\n",
    "        print(f\"Error: The Gradient Boosting model '{GB_MODEL_PATH}' was not found.\")\n",
    "        return\n",
    "    if not os.path.exists(TEST_DATA_PATH):\n",
    "        print(f\"Error: The test data file '{TEST_DATA_PATH}' was not found.\")\n",
    "        return\n",
    "    if not os.path.isdir(TINYBERT_MODEL_PATH):\n",
    "        print(f\"Error: The TinyBERT model directory '{TINYBERT_MODEL_PATH}' was not found.\")\n",
    "        return\n",
    "    \n",
    "    # Check for specific tokenizer files to prevent the 'NoneType' error\n",
    "    tokenizer_file_check = os.path.join(TINYBERT_MODEL_PATH, 'tokenizer.json')\n",
    "    if not os.path.exists(tokenizer_file_check):\n",
    "        print(f\"Error: The necessary tokenizer file '{tokenizer_file_check}' was not found.\")\n",
    "        print(\"Please ensure you have saved both the model and the tokenizer to this directory.\")\n",
    "        return\n",
    "\n",
    "    print(\"Loading all models and data...\")\n",
    "    meta_model = joblib.load(META_MODEL_PATH)\n",
    "    vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "    gb_model = joblib.load(GB_MODEL_PATH)\n",
    "    \n",
    "    # TinyBERT model and tokenizer from the local directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TINYBERT_MODEL_PATH)\n",
    "    tinybert_model = AutoModelForSequenceClassification.from_pretrained(TINYBERT_MODEL_PATH)\n",
    "    tinybert_model.eval()\n",
    "\n",
    "    test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "    print(f\"Loaded {len(test_df)} reviews for testing.\")\n",
    "\n",
    "    # --- 2. Prepare the Data for the Ensemble Pipeline ---\n",
    "    X_test_text = test_df['text']\n",
    "    y_true = test_df['violation_type']\n",
    "\n",
    "    # --- 3. Get Probability Predictions from the Base Models ---\n",
    "    print(\"Generating predictions from base models...\")\n",
    "\n",
    "    # Gradient Boosting Model\n",
    "    X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "    gb_probs = gb_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "    # TinyBERT Model\n",
    "    tinybert_probs = []\n",
    "    for text in X_test_text:\n",
    "        # --- THIS IS THE CRITICAL FIX ---\n",
    "        # Explicitly set max_length to match the model's capacity\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = tinybert_model(**inputs)\n",
    "        \n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()\n",
    "        tinybert_probs.append(probabilities[0])\n",
    "\n",
    "    tinybert_probs = pd.DataFrame(tinybert_probs)\n",
    "\n",
    "    # --- 4. Combine Probabilities and Make Final Prediction ---\n",
    "    X_meta_test = pd.concat([pd.DataFrame(gb_probs), tinybert_probs], axis=1)\n",
    "\n",
    "    print(\"Making final predictions with the RandomForest meta-model...\")\n",
    "    y_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "    # --- 5. Evaluate and Report Results ---\n",
    "    print(\"\\n--- Evaluation Report ---\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Overall Ensemble Accuracy on New Data: {accuracy:.4f}\\n\")\n",
    "\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"Full Classification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_ensemble_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34e6817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Ensemble Model Evaluation ---\n",
      "Loading all models and data...\n",
      "Loaded 800 reviews for testing.\n",
      "Generating predictions from base models...\n",
      "Making final predictions with the RandomForest meta-model...\n",
      "\n",
      "--- Evaluation Report ---\n",
      "Overall Ensemble Accuracy on New Data: 0.8850\n",
      "\n",
      "Full Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.94      0.94       200\n",
      "           2       0.83      0.88      0.85       202\n",
      "           3       0.98      0.86      0.92       201\n",
      "           4       0.81      0.86      0.83       197\n",
      "\n",
      "    accuracy                           0.89       800\n",
      "   macro avg       0.89      0.88      0.89       800\n",
      "weighted avg       0.89      0.89      0.89       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- File Paths and Constants ---\n",
    "# Meta-Model and Vectorizer from the ensemble tuning\n",
    "META_MODEL_PATH = '../models/ensemble_meta_model.pkl'\n",
    "VECTORIZER_PATH = '../models/vectorizer_advanced_tuned.pkl'\n",
    "\n",
    "# Base models (needed for their predictions)\n",
    "GB_MODEL_PATH = '../models/gb_model_advanced_tuned.pkl'\n",
    "# The path to your local TinyBERT model folder\n",
    "TINYBERT_MODEL_PATH = '../models/tinybertfinetuned'\n",
    "\n",
    "# Dataset to be tested\n",
    "TEST_DATA_PATH = '../data/augmented_labeled_reviews_vermont.csv'\n",
    "\n",
    "def test_ensemble_model():\n",
    "    \"\"\"\n",
    "    Loads the full ensemble pipeline, makes predictions on the pseudo-labeled\n",
    "    test data, and prints the performance report.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Ensemble Model Evaluation ---\")\n",
    "\n",
    "    # --- 1. Load the Necessary Components ---\n",
    "    # First, check if all the necessary files exist\n",
    "    if not os.path.exists(META_MODEL_PATH):\n",
    "        print(f\"Error: The meta-model file '{META_MODEL_PATH}' was not found.\")\n",
    "        return\n",
    "    if not os.path.exists(VECTORIZER_PATH):\n",
    "        print(f\"Error: The vectorizer file '{VECTORIZER_PATH}' was not found.\")\n",
    "        return\n",
    "    if not os.path.exists(GB_MODEL_PATH):\n",
    "        print(f\"Error: The Gradient Boosting model '{GB_MODEL_PATH}' was not found.\")\n",
    "        return\n",
    "    if not os.path.exists(TEST_DATA_PATH):\n",
    "        print(f\"Error: The test data file '{TEST_DATA_PATH}' was not found.\")\n",
    "        return\n",
    "    if not os.path.isdir(TINYBERT_MODEL_PATH):\n",
    "        print(f\"Error: The TinyBERT model directory '{TINYBERT_MODEL_PATH}' was not found.\")\n",
    "        return\n",
    "    \n",
    "    # Check for specific tokenizer files to prevent the 'NoneType' error\n",
    "    tokenizer_file_check = os.path.join(TINYBERT_MODEL_PATH, 'tokenizer.json')\n",
    "    if not os.path.exists(tokenizer_file_check):\n",
    "        print(f\"Error: The necessary tokenizer file '{tokenizer_file_check}' was not found.\")\n",
    "        print(\"Please ensure you have saved both the model and the tokenizer to this directory.\")\n",
    "        return\n",
    "\n",
    "    print(\"Loading all models and data...\")\n",
    "    meta_model = joblib.load(META_MODEL_PATH)\n",
    "    vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "    gb_model = joblib.load(GB_MODEL_PATH)\n",
    "    \n",
    "    # TinyBERT model and tokenizer from the local directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TINYBERT_MODEL_PATH)\n",
    "    tinybert_model = AutoModelForSequenceClassification.from_pretrained(TINYBERT_MODEL_PATH)\n",
    "    tinybert_model.eval()\n",
    "\n",
    "    test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "    print(f\"Loaded {len(test_df)} reviews for testing.\")\n",
    "\n",
    "    # --- 2. Prepare the Data for the Ensemble Pipeline ---\n",
    "    X_test_text = test_df['text']\n",
    "    y_true = test_df['violation_type']\n",
    "\n",
    "    # --- 3. Get Probability Predictions from the Base Models ---\n",
    "    print(\"Generating predictions from base models...\")\n",
    "\n",
    "    # Gradient Boosting Model\n",
    "    X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "    gb_probs = gb_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "    # TinyBERT Model\n",
    "    tinybert_probs = []\n",
    "    for text in X_test_text:\n",
    "        # Explicitly set max_length to match the model's capacity\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = tinybert_model(**inputs)\n",
    "        \n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()\n",
    "        tinybert_probs.append(probabilities[0])\n",
    "\n",
    "    tinybert_probs = np.array(tinybert_probs)\n",
    "\n",
    "    # --- 4. Combine Probabilities and Make Final Prediction ---\n",
    "    # FIX: Use np.hstack to combine the predictions, matching the training script\n",
    "    X_meta_test = np.hstack([gb_probs, tinybert_probs])\n",
    "\n",
    "    print(\"Making final predictions with the RandomForest meta-model...\")\n",
    "    y_pred = meta_model.predict(X_meta_test)\n",
    "    \n",
    "    # --- 5. Explicitly convert y_true to string to match y_pred ---\n",
    "    y_true_fixed = y_true.astype(str)\n",
    "\n",
    "    # --- 6. Evaluate and Report Results ---\n",
    "    print(\"\\n--- Evaluation Report ---\")\n",
    "    # Pass the fixed y_true to the evaluation functions\n",
    "    accuracy = accuracy_score(y_true_fixed, y_pred)\n",
    "    print(f\"Overall Ensemble Accuracy on New Data: {accuracy:.4f}\\n\")\n",
    "\n",
    "    report = classification_report(y_true_fixed, y_pred)\n",
    "    print(\"Full Classification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_ensemble_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
