{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736de235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ariff1422\\Documents\\Verity\\notebooks\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13446658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "REVIEW_FILE = f'../data/raw/review-California_10.json'\n",
    "META_FILE = f'../data/raw/meta-California.json'\n",
    "OUTPUT_FILE = f'../data/california_reviews_merged.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a76b64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator function to read in a large set of data especially the k-core dataset for California\n",
    "def read_json_lines(path, desc=\"Processing file\"):\n",
    "    \"\"\"Reads a JSON file where each line is a separate JSON object.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # Use tqdm to show a progress bar for large files\n",
    "        for line in tqdm(f, desc=desc):\n",
    "            try:\n",
    "                # Safely load each line as a JSON object\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip any lines that are not valid JSON\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8b02e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Processing California review data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading reviews: 44476890it [03:03, 242931.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews DataFrame created with 44476890 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Process Reviews Data ---\n",
    "print(\"Step 1: Processing California review data...\")\n",
    "review_fields = ['text', 'rating', 'time', 'user_id', 'gmap_id']\n",
    "reviews_data = []\n",
    "\n",
    "# Iterate over the generator and build a list of dictionaries\n",
    "for review in read_json_lines(REVIEW_FILE, desc=\"Reading reviews\"):\n",
    "    # Extract only the relevant fields to save memory\n",
    "    reviews_data.append({k: review.get(k) for k in review_fields})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "reviews_df = pd.DataFrame(reviews_data)\n",
    "print(f\"Reviews DataFrame created with {len(reviews_df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f46b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Processing business metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata: 515961it [00:21, 23999.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata DataFrame created with 515961 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Process Metadata Data ---\n",
    "print(\"\\nStep 2: Processing business metadata...\")\n",
    "meta_fields = ['gmap_id', 'category', 'description', 'avg_rating', 'num_of_reviews']\n",
    "metas_data = []\n",
    "\n",
    "# The metadata file is much smaller, so loading it at once is usually fine\n",
    "for meta in read_json_lines(META_FILE, desc=\"Reading metadata\"):\n",
    "    metas_data.append({k: meta.get(k) for k in meta_fields})\n",
    "\n",
    "meta_df = pd.DataFrame(metas_data)\n",
    "print(f\"Metadata DataFrame created with {len(meta_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68eba260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Merging and cleaning data...\n",
      "Initial merged DataFrame has 44504776 rows.\n",
      "After dropping duplicates, DataFrame has 43968127 rows.\n",
      "After dropping rows with missing text or category, DataFrame has 23258034 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Merge and Clean Data ---\n",
    "print(\"\\nStep 3: Merging and cleaning data...\")\n",
    "\n",
    "merged_df = pd.merge(reviews_df, meta_df, on='gmap_id', how='left')\n",
    "print(f\"Initial merged DataFrame has {len(merged_df)} rows.\")\n",
    "\n",
    "# FIX FOR THE ERROR: Convert the 'category' list to a string\n",
    "merged_df['category'] = merged_df['category'].astype(str)\n",
    "\n",
    "# Drop duplicate rows to ensure a clean dataset\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "print(f\"After dropping duplicates, DataFrame has {len(merged_df)} rows.\")\n",
    "\n",
    "# Drop rows where the 'text' or 'category' is missing\n",
    "merged_df.dropna(subset=['text', 'category'], inplace=True)\n",
    "print(f\"After dropping rows with missing text or category, DataFrame has {len(merged_df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6aeef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Saving the final processed file...\n",
      "Success! Final cleaned dataset saved to ../data/california_reviews_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Save the Final Dataset ---\n",
    "print(\"\\nStep 4: Saving the final processed file...\")\n",
    "merged_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Success! Final cleaned dataset saved to {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
