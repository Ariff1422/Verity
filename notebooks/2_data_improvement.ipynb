{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "485548f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# --- Try to import pycld2 as an alternative ---\n",
    "try:\n",
    "    import pycld2\n",
    "except ImportError:\n",
    "    print(\"Error: The 'pycld2' library is not installed.\")\n",
    "    print(\"Please install it using: pip install pycld2\")\n",
    "    # Define a dummy function to prevent further errors\n",
    "    def detect_pycld2_language(text):\n",
    "        return 'unknown'\n",
    "    class PyCLD2Exception(Exception):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59d82015",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 5\n",
    "BATCH_SIZE = 20000\n",
    "CONFIDENCE_THRESHOLD = 0.95\n",
    "\n",
    "# --- File Paths ---\n",
    "MERGED_DATA_PATH = os.path.join('..', 'data', 'california_reviews_merged.csv')\n",
    "OUTPUT_DATA_PATH = os.path.join('..', 'data', 'llm_labeled_subset_50k_self_trained.csv')\n",
    "LOCAL_LABELS_PATH = os.path.join('..', 'data', 'llm_subset_labels_local.csv')\n",
    "\n",
    "# --- Labeling Guidelines (Improved) ---\n",
    "# This dictionary contains keywords and regex patterns for better initial labeling.\n",
    "LOCAL_LABELS = {\n",
    "    'spam': {\n",
    "        'keywords': ['http', 'www', 'click here', 'sale', 'promo', 'discount', 'buy now', 'advertisement'],\n",
    "        'regex': [r'https?://\\S+', r'\\b(www\\.)\\S+'] # Catches URLs\n",
    "    },\n",
    "    'irrelevant': {\n",
    "        'keywords': ['movie', 'tv show', 'book', 'weather', 'news', 'nonsense', 'unrelated'],\n",
    "        'regex': [r'']\n",
    "    },\n",
    "    'policy_violation': {\n",
    "        'keywords': ['hate speech', 'fraud', 'illegal', 'threat', 'scam', 'ripoff'],\n",
    "        'regex': [r'']\n",
    "    },\n",
    "    'rant': {\n",
    "        'keywords': ['disgusting', 'terrible', 'horrible', 'trash', 'never again', 'worse', 'angry'],\n",
    "        'regex': [r'!!!+', r'!!!', r'!!!+'] # Excessive punctuation\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad70f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review_locally(review_text):\n",
    "    \"\"\"\n",
    "    Classifies a review based on improved keyword matching and regex patterns.\n",
    "    Returns an integer label (0, 1, 2, 3, or 4).\n",
    "    \"\"\"\n",
    "    text = str(review_text).lower()\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if pd.isna(review_text):\n",
    "        return 0\n",
    "\n",
    "    # Classify based on new, more robust rules\n",
    "    if any(keyword in text for keyword in LOCAL_LABELS['spam']['keywords']) or \\\n",
    "       any(re.search(pattern, text) for pattern in LOCAL_LABELS['spam']['regex']):\n",
    "        return 1\n",
    "    elif any(keyword in text for keyword in LOCAL_LABELS['irrelevant']['keywords']):\n",
    "        return 2\n",
    "    elif any(keyword in text for keyword in LOCAL_LABELS['policy_violation']['keywords']):\n",
    "        return 3\n",
    "    elif any(keyword in text for keyword in LOCAL_LABELS['rant']['keywords']) or \\\n",
    "         any(re.search(pattern, text) for pattern in LOCAL_LABELS['rant']['regex']) or \\\n",
    "         (sum(1 for c in review_text if c.isupper()) / len(review_text) > 0.5 if len(review_text) > 0 else False): # High percentage of caps\n",
    "        return 4\n",
    "    else:\n",
    "        return 0 # Valid Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3db4342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Detects if a given text is in English using the pycld2 library.\n",
    "    Returns True if English with high confidence, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # pycld2 returns a tuple: is_reliable, bytes_consumed, languages\n",
    "        # The languages list contains a tuple of (language_name, language_code, score)\n",
    "        is_reliable, _, languages = pycld2.detect(text)\n",
    "        language_code = languages[0][1] # Get the code of the most likely language\n",
    "        \n",
    "        # Check if the detected language is English and if the detection is reliable\n",
    "        return language_code == 'en' and is_reliable\n",
    "    except Exception:\n",
    "        # Return False on any error during detection\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d6b351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Filtering non-English reviews ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Languages: 100%|██████████| 23258034/23258034 [03:47<00:00, 102403.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 23258034 reviews.\n",
      "Filtered dataset size: 20938426 English reviews.\n",
      "Language filtering took 238.13 seconds.\n",
      "\n",
      "Starting initial local labeling for 500 reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local Labeling: 100%|██████████| 500/500 [00:00<00:00, 11723.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initial Local Labeling Complete! ---\n",
      "Total time elapsed: 0.05 seconds.\n",
      "\n",
      "Proceeding with initial labeled data. Labeled dataset size is: 500\n",
      "Distribution of initial labels:\n",
      "violation_type\n",
      "0    466\n",
      "4     22\n",
      "1      6\n",
      "2      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Initializing Sentence-BERT and Logistic Regression models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Self-Training Iteration 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 13/13 [00:01<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Accuracy on test set: 0.93\n",
      "Current labeled dataset size: 500\n",
      "\n",
      "Predicting on a new batch of 20000 unlabeled reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 625/625 [01:25<00:00,  7.30it/s]\n",
      "Detecting Languages:  80%|████████  | 18658347/23258034 [44:48<11:02, 6939.27it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 8118 confident samples to the training data.\n",
      "New labeled dataset size: 8618\n",
      "\n",
      "--- Self-Training Iteration 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 216/216 [00:40<00:00,  5.28it/s]\n",
      "Batches: 100%|██████████| 54/54 [00:18<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Accuracy on test set: 1.00\n",
      "Current labeled dataset size: 8618\n",
      "\n",
      "Predicting on a new batch of 20000 unlabeled reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 625/625 [03:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 19102 confident samples to the training data.\n",
      "New labeled dataset size: 27720\n",
      "\n",
      "--- Self-Training Iteration 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 693/693 [03:24<00:00,  3.39it/s]\n",
      "Batches: 100%|██████████| 174/174 [00:55<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Accuracy on test set: 1.00\n",
      "Current labeled dataset size: 27720\n",
      "\n",
      "Predicting on a new batch of 20000 unlabeled reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 625/625 [04:05<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 19999 confident samples to the training data.\n",
      "New labeled dataset size: 47719\n",
      "\n",
      "--- Self-Training Iteration 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1193/1193 [06:03<00:00,  3.29it/s]\n",
      "Batches: 100%|██████████| 299/299 [02:03<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Accuracy on test set: 1.00\n",
      "Current labeled dataset size: 47719\n",
      "\n",
      "Predicting on a new batch of 20000 unlabeled reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 625/625 [04:16<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 20000 confident samples to the training data.\n",
      "New labeled dataset size: 67719\n",
      "\n",
      "--- Self-Training Iteration 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1693/1693 [08:29<00:00,  3.32it/s]\n",
      "Batches: 100%|██████████| 424/424 [02:02<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Accuracy on test set: 1.00\n",
      "Current labeled dataset size: 67719\n",
      "\n",
      "Predicting on a new batch of 20000 unlabeled reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 625/625 [03:49<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 20000 confident samples to the training data.\n",
      "New labeled dataset size: 87719\n",
      "\n",
      "--- Self-Training Complete! ---\n",
      "Final labeled dataset size: 87719\n",
      "\n",
      "Final labeled dataset saved to: ..\\data\\llm_labeled_subset_50k_self_trained.csv\n",
      "\n",
      "Distribution of Model-assigned labels on the final dataset:\n",
      "violation_type\n",
      "0    87685\n",
      "4       22\n",
      "1        6\n",
      "2        6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "This labeled dataset is now ready for final model training or analysis.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 1. Load the data with performance optimizations ---\n",
    "    try:\n",
    "        column_dtypes = {\n",
    "            'text': 'string',\n",
    "            'user_id': 'string',\n",
    "            'gmap_id': 'string',\n",
    "        }\n",
    "        df = pd.read_csv(\n",
    "            MERGED_DATA_PATH,\n",
    "            on_bad_lines='skip',\n",
    "            engine='c'\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {MERGED_DATA_PATH}. Please ensure the data folder is in the correct location.\")\n",
    "        exit()\n",
    "    \n",
    "    # --- 1.5: Filter out non-English reviews ---\n",
    "    print(\"\\n--- Filtering non-English reviews ---\")\n",
    "    start_time_filter = time.time()\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Apply the language detection function to the 'text' column\n",
    "    english_reviews_mask = [is_english(text) for text in tqdm(df['text'].tolist(), desc=\"Detecting Languages\")]\n",
    "    df = df[english_reviews_mask].copy()\n",
    "    \n",
    "    end_time_filter = time.time()\n",
    "    print(f\"Original dataset size: {initial_count} reviews.\")\n",
    "    print(f\"Filtered dataset size: {len(df)} English reviews.\")\n",
    "    print(f\"Language filtering took {end_time_filter - start_time_filter:.2f} seconds.\")\n",
    "\n",
    "    # --- 2. Take a random sample for initial local labeling ---\n",
    "    SAMPLE_SIZE_LOCAL_LABELS = 500\n",
    "    labeled_df = df.sample(n=SAMPLE_SIZE_LOCAL_LABELS, random_state=42).copy()\n",
    "    unlabeled_df = df.drop(labeled_df.index)\n",
    "    labeled_df['violation_type'] = -1\n",
    "\n",
    "    print(f\"\\nStarting initial local labeling for {SAMPLE_SIZE_LOCAL_LABELS} reviews...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for index, row in tqdm(labeled_df.iterrows(), total=len(labeled_df), desc=\"Local Labeling\"):\n",
    "        labeled_df.at[index, 'violation_type'] = classify_review_locally(row['text'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n--- Initial Local Labeling Complete! ---\")\n",
    "    print(f\"Total time elapsed: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # --- 3. Augment the initial labeled data ---\n",
    "    print(f\"\\nProceeding with initial labeled data. Labeled dataset size is: {len(labeled_df)}\")\n",
    "    print(\"Distribution of initial labels:\")\n",
    "    print(labeled_df['violation_type'].value_counts())\n",
    "\n",
    "    # --- 4. Initialize the models ---\n",
    "    print(\"\\nInitializing Sentence-BERT and Logistic Regression models...\")\n",
    "    model_emb = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    clf = LogisticRegression(max_iter=2000)\n",
    "    \n",
    "    # --- 5. Begin the self-training loop ---\n",
    "    for i in range(NUM_ITERATIONS):\n",
    "        print(f\"\\n--- Self-Training Iteration {i+1}/{NUM_ITERATIONS} ---\")\n",
    "        \n",
    "        # Split the current labeled data into train/test sets for evaluation\n",
    "        X_labeled = labeled_df.copy()\n",
    "        y_labeled = labeled_df['violation_type']\n",
    "        \n",
    "        if y_labeled.nunique() > 1 and all(y_labeled.value_counts() > 1):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42, stratify=y_labeled)\n",
    "        else:\n",
    "            print(\"Cannot perform stratified split. Not enough samples in some classes.\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Encode the labeled text data\n",
    "        X_train_emb_text = model_emb.encode(X_train['text'].tolist(), show_progress_bar=True)\n",
    "        X_test_emb_text = model_emb.encode(X_test['text'].tolist(), show_progress_bar=True)\n",
    "        \n",
    "        clf.fit(X_train_emb_text, y_train)\n",
    "        \n",
    "        accuracy = clf.score(X_test_emb_text, y_test)\n",
    "        print(f\"Model trained. Accuracy on test set: {accuracy:.2f}\")\n",
    "        print(f\"Current labeled dataset size: {len(labeled_df)}\")\n",
    "        \n",
    "        if len(unlabeled_df) < BATCH_SIZE:\n",
    "            print(\"Not enough unlabeled data for a full batch. Using remaining data.\")\n",
    "            batch_df = unlabeled_df.copy()\n",
    "            unlabeled_df = pd.DataFrame()\n",
    "        else:\n",
    "            batch_df = unlabeled_df.sample(n=BATCH_SIZE, random_state=42).copy()\n",
    "            unlabeled_df = unlabeled_df.drop(batch_df.index)\n",
    "\n",
    "        batch_df.dropna(subset=['text'], inplace=True)\n",
    "        \n",
    "        print(f\"\\nPredicting on a new batch of {len(batch_df)} unlabeled reviews...\")\n",
    "        X_batch_emb_text = model_emb.encode(batch_df['text'].tolist(), show_progress_bar=True)\n",
    "        \n",
    "        batch_preds = clf.predict(X_batch_emb_text)\n",
    "        batch_probs = clf.predict_proba(X_batch_emb_text)\n",
    "        \n",
    "        batch_max_probs = np.max(batch_probs, axis=1)\n",
    "        \n",
    "        confident_indices = np.where(batch_max_probs > CONFIDENCE_THRESHOLD)[0]\n",
    "        \n",
    "        confident_df = batch_df.iloc[confident_indices].copy()\n",
    "        confident_df['violation_type'] = batch_preds[confident_indices]\n",
    "        \n",
    "        labeled_df = pd.concat([labeled_df, confident_df], ignore_index=True)\n",
    "        \n",
    "        print(f\"Added {len(confident_df)} confident samples to the training data.\")\n",
    "        print(f\"New labeled dataset size: {len(labeled_df)}\")\n",
    "        \n",
    "    # --- 6. Final labeling and saving ---\n",
    "    print(\"\\n--- Self-Training Complete! ---\")\n",
    "    print(f\"Final labeled dataset size: {len(labeled_df)}\")\n",
    "    \n",
    "    labeled_df.to_csv(OUTPUT_DATA_PATH, index=False)\n",
    "    print(f\"\\nFinal labeled dataset saved to: {OUTPUT_DATA_PATH}\")\n",
    "    print(\"\\nDistribution of Model-assigned labels on the final dataset:\")\n",
    "    print(labeled_df['violation_type'].value_counts())\n",
    "    print(\"\\nThis labeled dataset is now ready for final model training or analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aea148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
